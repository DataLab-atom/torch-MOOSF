{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import argparse, os, shutil, time, random, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import losses\n",
    "from datasets.cifar100 import get_cifar100\n",
    "from train.train import get_train_fn, get_update_score_fn\n",
    "from train.validate import get_valid_fn\n",
    "from models.net import get_model\n",
    "from losses.loss import get_loss, get_optimizer\n",
    "from utils.config import parse_args, reproducibility, dataset_argument\n",
    "from utils.common import make_imb_data, save_checkpoint, adjust_learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(run_type = 'jupyter')\n",
    "reproducibility(args.seed)\n",
    "args = dataset_argument(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.imb_ratio = 100\n",
    "args.loss_fn = 'ride'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 477, 455, 434, 415, 396, 378, 361, 344, 328, 314, 299, 286, 273, 260, 248, 237, 226, 216, 206, 197, 188, 179, 171, 163, 156, 149, 142, 135, 129, 123, 118, 112, 107, 102, 98, 93, 89, 85, 81, 77, 74, 70, 67, 64, 61, 58, 56, 53, 51, 48, 46, 44, 42, 40, 38, 36, 35, 33, 32, 30, 29, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 15, 14, 13, 13, 12, 12, 11, 11, 10, 10, 9, 9, 8, 8, 7, 7, 7, 6, 6, 6, 6, 5, 5, 5, 5]\n",
      "==> Preparing imbalanced CIFAR-100\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Magnitude set = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], dtype=torch.int32)\n",
      "Operation set = tensor([ 0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,\n",
      "         9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15], dtype=torch.int32)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "#Train: 10847, #Test: 9000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert args.num_max <= 50000. / args.num_class\n",
    "except AssertionError:\n",
    "    args.num_max = int(50000 / args.num_class)\n",
    "\n",
    "N_SAMPLES_PER_CLASS = make_imb_data(args.num_max, args.num_class, args.imb_ratio)\n",
    "\n",
    "if args.dataset == 'cifar100':\n",
    "    print(f'==> Preparing imbalanced CIFAR-100')\n",
    "    trainset, allset, devset, testset = get_cifar100(os.path.join(args.data_dir, 'cifar100/'), N_SAMPLES_PER_CLASS, cutout = args.cutout,  num_test = args.num_test, seed = args.seed)\n",
    "    \n",
    "trainloader = data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last=False)\n",
    "\n",
    "allloader = data.DataLoader(allset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "devloader = data.DataLoader(devset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "testloader = data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix augment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> creating resnet32\n",
      "    Total params: 0.79M\n",
      "Max state: 1 // Min state: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6580/2761659037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mall_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_SAMPLES_PER_CLASS\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'All Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_SAMPLES_PER_CLASS\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dev Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_SAMPLES_PER_CLASS\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/input/sumyeong/CUDA/CUDA/train/validate.py\u001b[0m in \u001b[0;36mvalid_normal\u001b[0;34m(args, valloader, model, criterion, per_class_num, num_class, mode)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# measure accuracy and record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mprec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mtop1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mtop5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print (\"==> creating {}\".format(args.network))\n",
    "model = get_model(args, N_SAMPLES_PER_CLASS)\n",
    "train_criterion = get_loss(args, N_SAMPLES_PER_CLASS)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none') # For test, validation \n",
    "optimizer = get_optimizer(args, model)\n",
    "train = get_train_fn(args)\n",
    "update_score = get_update_score_fn(args)\n",
    "validate = get_valid_fn(args)\n",
    "\n",
    "\n",
    "############ Initial Training\n",
    "test_accs, dev_accs, all_accs = [],[],[]\n",
    "# for epoch in range(args.epochs):\n",
    "for epoch in range(args.epochs):\n",
    "    lr = adjust_learning_rate(optimizer, epoch, args)\n",
    "    curr_state, label = update_score(trainloader, model, num_test = args.num_test)\n",
    "    train_loss = train(args, trainloader, model, optimizer, train_criterion, epoch)\n",
    "\n",
    "\n",
    "    all_loss, all_acc, all_cls = validate(args, allloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='All Valid')\n",
    "    dev_loss, dev_acc, dev_cls = validate(args, devloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='Dev Valid')\n",
    "    test_loss, test_acc, test_cls = validate(args, testloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='Test Valid')\n",
    "\n",
    "    print(f'Epoch: [{epoch+1} | {args.epochs}]')\n",
    "    print(f'LR: {lr}')\n",
    "    print(f'[Train]\\tLoss:\\t{train_loss:.4f}')\n",
    "    print(f'[Dev set]\\tLoss:\\t{dev_loss:.4f}\\tAcc:\\t{dev_acc:.4f}')\n",
    "    print(f'[Test set]\\tLoss:\\t{test_loss:.4f}\\tAcc:\\t{test_acc:.4f}')\n",
    "    print(f'[All set]\\tLoss:\\t{all_loss:.4f}\\tAcc:\\t{all_acc:.4f}')\n",
    "    print(f'[Dev Cls]\\tMany:\\t{dev_cls[0]:.4f}\\tMedium:\\t{dev_cls[1]:.4f}\\tFew:\\t{dev_cls[2]:.4f}')\n",
    "    print(f'[Test Cls]\\tMany:\\t{test_cls[0]:.4f}\\tMedium:\\t{test_cls[1]:.4f}\\tFew:\\t{test_cls[2]:.4f}')\n",
    "    print(f'[All Cls]\\tMany:\\t{all_cls[0]:.4f}\\tMedium:\\t{all_cls[1]:.4f}\\tFew:\\t{all_cls[2]:.4f}')\n",
    "\n",
    "\n",
    "# Print the final results\n",
    "print(f'Final Performance...')\n",
    "print(f'best bAcc (test): {np.max(test_accs)}')\n",
    "print(f'best bAcc (dev): {np.max(dev_accs)}')\n",
    "print(f'best bAcc (all): {np.max(all_accs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print (\"==> creating {}\".format(args.network))\n",
    "model = get_model(args, N_SAMPLES_PER_CLASS)\n",
    "train_criterion = get_loss(args, N_SAMPLES_PER_CLASS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = get_optimizer(args, model)\n",
    "train = get_train_fn(args)\n",
    "validate = get_valid_fn(args)\n",
    "\n",
    "test_accs, dev_accs, all_accs = [],[],[]\n",
    "# for epoch in range(args.epochs):\n",
    "for epoch in range(args.epochs):\n",
    "    lr = adjust_learning_rate(optimizer, epoch, args)\n",
    "    train_loss = train(args, trainloader, model, optimizer, train_criterion, epoch)\n",
    "\n",
    "    all_loss, all_acc, all_cls = validate(args, allloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='All Valid')\n",
    "    dev_loss, dev_acc, dev_cls = validate(args, devloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='Dev Valid')\n",
    "    test_loss, test_acc, test_cls = validate(args, testloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='Test Valid')\n",
    "\n",
    "    print(f'Epoch: [{epoch+1} | {args.epochs}]')\n",
    "    print(f'LR: {lr}')\n",
    "    print(f'[Train]\\tLoss:\\t{train_loss:.4f}')\n",
    "    print(f'[Dev set]\\tLoss:\\t{dev_loss:.4f}\\tAcc:\\t{dev_acc:.4f}')\n",
    "    print(f'[Test set]\\tLoss:\\t{test_loss:.4f}\\tAcc:\\t{test_acc:.4f}')\n",
    "    print(f'[All set]\\tLoss:\\t{all_loss:.4f}\\tAcc:\\t{all_acc:.4f}')\n",
    "    print(f'[Dev Cls]\\tMany:\\t{dev_cls[0]:.4f}\\tMedium:\\t{dev_cls[1]:.4f}\\tFew:\\t{dev_cls[2]:.4f}')\n",
    "    print(f'[Test Cls]\\tMany:\\t{test_cls[0]:.4f}\\tMedium:\\t{test_cls[1]:.4f}\\tFew:\\t{test_cls[2]:.4f}')\n",
    "    print(f'[All Cls]\\tMany:\\t{all_cls[0]:.4f}\\tMedium:\\t{all_cls[1]:.4f}\\tFew:\\t{all_cls[2]:.4f}')\n",
    "\n",
    "    test_accs.append(test_acc)\n",
    "    dev_accs.append(dev_acc)\n",
    "    all_accs.append(all_acc)\n",
    "\n",
    "# Print the final results\n",
    "print(f'Final Performance...')\n",
    "print(f'best bAcc (test): {np.max(test_accs)}')\n",
    "print(f'best bAcc (dev): {np.max(dev_accs)}')\n",
    "print(f'best bAcc (all): {np.max(all_accs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
